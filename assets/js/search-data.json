{
  
    
        "post0": {
            "title": "Data Structures- Hashmaps, Sets, Hash Tables, Hashing and Collisions",
            "content": "What is a Hashtable/Hashmap? . A hashtable is a data structure that with a collection of key-value pairs, where each key maps to a value, and the keys must be unique and hashable. . In Python there is a built in hashtable known as a ___. | . The primary purpose of a hashtable is to provide efficient lookup, insertion, and deletion operations. When an element is to be inserted into the hashtable, a hash function is used to map the key to a specific index in the underlying array that is used to store the key-value pairs. The value is then stored at that index. When searching for a value, the hash function is used again to find the index where the value is stored. . The key advantage of a hashtable over other data structures like arrays and linked lists is its average-case time complexity for lookup, insertion, and deletion operations. . The typical time complexity of a hashtable is ___. | . What is Hashing and Collision? . Hashing is the process of mapping a given key to a value in a hash table or hashmap, using a hash function. The hash function takes the key as input and produces a hash value or hash code, which is then used to determine the index in the underlying array where the value is stored. The purpose of hashing is to provide a quick and efficient way to access data, by eliminating the need to search through an entire data structure to find a value. . However, it is possible for two different keys to map to the same hash value, resulting in a collision. When a collision occurs, there are different ways to resolve it, depending on the collision resolution strategy used. . Python&#39;s dictionary implementation is optimized to handle collisions efficiently, and the performance of the dictionary is generally very good, even in the presence of collisions. However, if the number of collisions is very high, the performance of the dictionary can degrade, so it is important to choose a good hash function that minimizes collisions when designing a Python dictionary. . What is a Set? . my_set = set([1, 2, 3, 2, 1]) print(my_set) # What do you notice in the output? # # # Why do you think Sets are in the same tech talk as Hashmaps/Hashtables? # # . Dictionary Example . Below are just some basic features of a dictionary. As always, documentation is always the main source for all the full capablilties. . lover_album = { &quot;title&quot;: &quot;Lover&quot;, &quot;artist&quot;: &quot;Taylor Swift&quot;, &quot;year&quot;: 2019, &quot;genre&quot;: [&quot;Pop&quot;, &quot;Synth-pop&quot;], &quot;tracks&quot;: { 1: &quot;I Forgot That You Existed&quot;, 2: &quot;Cruel Summer&quot;, 3: &quot;Lover&quot;, 4: &quot;The Man&quot;, 5: &quot;The Archer&quot;, 6: &quot;I Think He Knows&quot;, 7: &quot;Miss Americana &amp; The Heartbreak Prince&quot;, 8: &quot;Paper Rings&quot;, 9: &quot;Cornelia Street&quot;, 10: &quot;Death By A Thousand Cuts&quot;, 11: &quot;London Boy&quot;, 12: &quot;Soon You&#39;ll Get Better (feat. Dixie Chicks)&quot;, 13: &quot;False God&quot;, 14: &quot;You Need To Calm Down&quot;, 15: &quot;Afterglow&quot;, 16: &quot;Me! (feat. Brendon Urie of Panic! At The Disco)&quot;, 17: &quot;It&#39;s Nice To Have A Friend&quot;, 18: &quot;Daylight&quot; } } # What data structures do you see? # # # Printing the dictionary print(lover_album) . {&#39;title&#39;: &#39;Lover&#39;, &#39;artist&#39;: &#39;Taylor Swift&#39;, &#39;year&#39;: 2019, &#39;genre&#39;: [&#39;Pop&#39;, &#39;Synth-pop&#39;], &#39;tracks&#39;: {1: &#39;I Forgot That You Existed&#39;, 2: &#39;Cruel Summer&#39;, 3: &#39;Lover&#39;, 4: &#39;The Man&#39;, 5: &#39;The Archer&#39;, 6: &#39;I Think He Knows&#39;, 7: &#39;Miss Americana &amp; The Heartbreak Prince&#39;, 8: &#39;Paper Rings&#39;, 9: &#39;Cornelia Street&#39;, 10: &#39;Death By A Thousand Cuts&#39;, 11: &#39;London Boy&#39;, 12: &#34;Soon You&#39;ll Get Better (feat. Dixie Chicks)&#34;, 13: &#39;False God&#39;, 14: &#39;You Need To Calm Down&#39;, 15: &#39;Afterglow&#39;, 16: &#39;Me! (feat. Brendon Urie of Panic! At The Disco)&#39;, 17: &#34;It&#39;s Nice To Have A Friend&#34;, 18: &#39;Daylight&#39;}} . print(lover_album.get(&#39;tracks&#39;)) # or print(lover_album[&#39;tracks&#39;]) . print(lover_album.get(&#39;tracks&#39;)[4]) # or print(lover_album[&#39;tracks&#39;][4]) . lover_album[&quot;producer&quot;] = [&#39;Taylor Swift&#39;, &#39;Jack Antonoff&#39;, &#39;Joel Little&#39;, &#39;Taylor Swift&#39;, &#39;Louis Bell&#39;, &#39;Frank Dukes&#39;] # What can you change to make sure there are no duplicate producers? # # # Printing the dictionary print(lover_album) . lover_album[&quot;tracks&quot;].update({19: &quot;All Of The Girls You Loved Before&quot;}) # How would add an additional genre to the dictionary, like electropop? # # # Printing the dictionary print(lover_album) . for k,v in lover_album.items(): # iterate using a for loop for key and value print(str(k) + &quot;: &quot; + str(v)) # Write your own code to print tracks in readable format # # . def search(): search = input(&quot;What would you like to know about the album?&quot;) if lover_album.get(search.lower()) == None: print(&quot;Invalid Search&quot;) else: print(lover_album.get(search.lower())) search() # This is a very basic code segment, how can you improve upon this code? # # . Hacks . Answer ALL questions in the code segments | Create a diagram or comparison illustration (Canva). What are the pro and cons of using this data structure? | Dictionary vs List | . | Expand upon the code given to you, possible improvements in comments | Build your own album showing features of a python dictionary . | For Mr. Yeung&#39;s class: Justify your favorite Taylor Swift song, answer may effect seed . | .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/29/DS-hashmaps.html",
            "relUrl": "/2023/03/29/DS-hashmaps.html",
            "date": " • Mar 29, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Structures- Space and Time Complexity",
            "content": "Space and Time Complexity . Space complexity refers to the amount of memory used by an algorithm to complete its execution, as a function of the size of the input. The space complexity of an algorithm can be affected by various factors such as the size of the input data, the data structures used in the algorithm, the number and size of temporary variables, and the recursion depth. Time complexity refers to the amount of time required by an algorithm to run as the input size grows. It is usually measured in terms of the &quot;Big O&quot; notation, which describes the upper bound of an algorithm&#39;s time complexity. . Why do you think a programmer should care about space and time complexity? . ? | . Take a look at our lassen volcano example from the data compression tech talk. The first code block is the original image. In the second code block, change the baseWidth to rescale the image. . from IPython.display import Image, display from pathlib import Path # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) if __name__ == &quot;__main__&quot;: lassen_volcano = image_data(images=[{&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;}]) image_display(lassen_volcano) . from IPython.display import HTML, display from pathlib import Path from PIL import Image as pilImage from io import BytesIO import base64 # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def scale_image(img): #baseWidth = 625 #baseWidth = 1250 #baseWidth = 2500 baseWidth = 5000 # see the effect of doubling or halfing the baseWidth #baseWidth = 10000 #baseWidth = 20000 #baseWidth = 40000 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size image[&#39;width&#39;], image[&#39;height&#39;] = img.size image[&#39;pixels&#39;] = image[&#39;width&#39;] * image[&#39;height&#39;] # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size image[&#39;scaled_width&#39;], image[&#39;scaled_height&#39;] = img.size image[&#39;scaled_pixels&#39;] = image[&#39;scaled_width&#39;] * image[&#39;scaled_height&#39;] # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data(images = [{&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;}]) # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;], &quot; pixels: &quot;, f&quot;{image[&#39;pixels&#39;]:,}&quot;) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;], &quot; pixels: &quot;, f&quot;{image[&#39;scaled_pixels&#39;]:,}&quot;) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) . Do you think this is a time complexity or space complexity or both problem? . ? | . Big O Notation . Constant O(1) | Linear O(n) | Quadratic O(n^2) | Logarithmic O(logn) | Exponential (O(2^n)) | . numbers = list(range(1000)) print(numbers) . Constant O(1) . Time . An example of a constant time algorithm is accessing a specific element in an array. It does not matter how large the array is, accessing an element in the array takes the same amount of time. Therefore, the time complexity of this operation is constant, denoted by O(1). . print(numbers[263]) ncaa_bb_ranks = {1:&quot;Alabama&quot;,2:&quot;Houston&quot;, 3:&quot;Purdue&quot;, 4:&quot;Kansas&quot;} #look up a value in a dictionary given a key print(ncaa_bb_ranks[1]) . Space . This function takes two number inputs and returns their sum. The function does not create any additional data structures or variables that are dependent on the input size, so its space complexity is constant, or O(1). Regardless of how large the input numbers are, the function will always require the same amount of memory to execute. . def sum(a, b): return a + b print(sum(90,88)) print(sum(.9,.88)) . Linear O(n) . Time . An example of a linear time algorithm is traversing a list or an array. When the size of the list or array increases, the time taken to traverse it also increases linearly with the size. Hence, the time complexity of this operation is O(n), where n is the size of the list or array being traversed. . for i in numbers: print(i) . Space . This function takes a list of elements arr as input and returns a new list with the elements in reverse order. The function creates a new list reversed_arr of the same size as arr to store the reversed elements. The size of reversed_arr depends on the size of the input arr, so the space complexity of this function is O(n). As the input size increases, the amount of memory required to execute the function also increases linearly. . def reverse_list(arr): n = len(arr) reversed_arr = [None] * n #create a list of None based on the length or arr for i in range(n): reversed_arr[n-i-1] = arr[i] #stores the value at the index of arr to the value at the index of reversed_arr starting at the beginning for arr and end for reversed_arr return reversed_arr print(numbers) print(reverse_list(numbers)) . Quadratic O(n^2) . Time . An example of a quadratic time algorithm is nested loops. When there are two nested loops that both iterate over the same collection, the time taken to complete the algorithm grows quadratically with the size of the collection. Hence, the time complexity of this operation is O(n^2), where n is the size of the collection being iterated over. . for i in numbers: for j in numbers: print(i,j) . Space . This function takes two matrices matrix1 and matrix2 as input and returns their product as a new matrix. The function creates a new matrix result with dimensions m by n to store the product of the input matrices. The size of result depends on the size of the input matrices, so the space complexity of this function is O(n^2). As the size of the input matrices increases, the amount of memory required to execute the function also increases quadratically. . . Main take away is that a new matrix is created. | . def multiply_matrices(matrix1, matrix2): m = len(matrix1) n = len(matrix2[0]) result = [[0] * n] * m #this creates the new matrix based on the size of matrix 1 and 2 for i in range(m): for j in range(n): for k in range(len(matrix2)): result[i][j] += matrix1[i][k] * matrix2[k][j] return result print(multiply_matrices([[1,2],[3,4]], [[3,4],[1,2]])) . Logarithmic O(logn) . Time . An example of a log time algorithm is binary search. Binary search is an algorithm that searches for a specific element in a sorted list by repeatedly dividing the search interval in half. As a result, the time taken to complete the search grows logarithmically with the size of the list. Hence, the time complexity of this operation is O(log n), where n is the size of the list being searched. . def binary_search(arr, low, high, target): while low &lt;= high: mid = (low + high) // 2 #integer division if arr[mid] == target: return mid elif arr[mid] &lt; target: low = mid + 1 else: high = mid - 1 target = 263 result = binary_search(numbers, 0, len(numbers) - 1, target) print(result) . NameError Traceback (most recent call last) /Users/disvs/vscode/butterflyfish/_notebooks/2023-03-22-DS-space_time_complexity.ipynb Cell 28 in &lt;cell line: 13&gt;() &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/butterflyfish/_notebooks/2023-03-22-DS-space_time_complexity.ipynb#X36sZmlsZQ%3D%3D?line=9&#39;&gt;10&lt;/a&gt; high = mid - 1 &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/butterflyfish/_notebooks/2023-03-22-DS-space_time_complexity.ipynb#X36sZmlsZQ%3D%3D?line=11&#39;&gt;12&lt;/a&gt; target = 263 &gt; &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/butterflyfish/_notebooks/2023-03-22-DS-space_time_complexity.ipynb#X36sZmlsZQ%3D%3D?line=12&#39;&gt;13&lt;/a&gt; result = binary_search(numbers, 0, len(numbers) - 1, target) &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/butterflyfish/_notebooks/2023-03-22-DS-space_time_complexity.ipynb#X36sZmlsZQ%3D%3D?line=14&#39;&gt;15&lt;/a&gt; print(result) NameError: name &#39;numbers&#39; is not defined . Space . The same algorithm above has a O(logn) space complexity. The function takes an array arr, its lower and upper bounds low and high, and a target value target. The function searches for target within the bounds of arr by recursively dividing the search space in half until the target is found or the search space is empty. The function does not create any new data structures that depend on the size of arr. Instead, the function uses the call stack to keep track of the recursive calls. Since the maximum depth of the recursive calls is O(logn), where n is the size of arr, the space complexity of this function is O(logn). As the size of arr increases, the amount of memory required to execute the function grows logarithmically. . Exponential O(2^n) . Time . An example of an O(2^n) algorithm is the recursive implementation of the Fibonacci sequence. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, starting from 0 and 1. The recursive implementation of the Fibonacci sequence calculates each number by recursively calling itself with the two preceding numbers until it reaches the base case (i.e., the first or second number in the sequence). The algorithm takes O(2^n) time in the worst case because it has to calculate each number in the sequence by making two recursive calls. . . def fibonacci(n): if n &lt;= 1: return n else: return fibonacci(n-1) + fibonacci(n-2) #print(fibonacci(5)) #print(fibonacci(10)) #print(fibonacci(20)) print(fibonacci(30)) #print(fibonacci(40)) . 832040 . Space . This function takes a set s as input and generates all possible subsets of s. The function does this by recursively generating the subsets of the set without the first element, and then adding the first element to each of those subsets to generate the subsets that include the first element. The function creates a new list for each recursive call that stores the subsets, and each element in the list is a new list that represents a subset. The number of subsets that can be generated from a set of size n is 2^n, so the space complexity of this function is O(2^n). As the size of the input set increases, the amount of memory required to execute the function grows exponentially. . def generate_subsets(s): if not s: return [[]] subsets = generate_subsets(s[1:]) return [[s[0]] + subset for subset in subsets] + subsets print(generate_subsets([1,2,3])) #print(generate_subsets(numbers)) . Using the time library, we are able to see the difference in time it takes to calculate the fibonacci function above. . Based on what is known about the other time complexities, hypothesize the resulting elapsed time if the function is replaced. | . import time start_time = time.time() print(fibonacci(34)) end_time = time.time() total_time = end_time - start_time print(&quot;Time taken:&quot;, total_time, &quot;seconds&quot;) start_time = time.time() print(fibonacci(35)) end_time = time.time() total_time = end_time - start_time print(&quot;Time taken:&quot;, total_time, &quot;seconds&quot;) . 5702887 Time taken: 1.529968023300171 seconds 9227465 Time taken: 2.4437530040740967 seconds . Hacks . Record your findings when testing the time elapsed of the different algorithms. | Although we will go more in depth later, time complexity is a key concept that relates to the different sorting algorithms. Do some basic research on the different types of sorting algorithms and their time complexity. | Why is time and space complexity important when choosing an algorithm? | Should you always use a constant time algorithm / Should you never use an exponential time algorithm? Explain? | What are some general patterns that you noticed to determine each algorithm&#39;s time and space complexity? | . Complete the Time and Space Complexity analysis questions linked below. Practice .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/22/DS-space_time_complexity.html",
            "relUrl": "/2023/03/22/DS-space_time_complexity.html",
            "date": " • Mar 22, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Unit 2.4b Using Programs with Data, SQL",
            "content": "Database Programming is Program with Data . The Tri 2 Final Project is an example of a Program with Data. . Prepare to use SQLite in common Imperative Technique . Explore SQLite Connect object to establish database connection- Explore SQLite Cursor Object to fetch data from a table within a database | . Schema of Users table in Sqlite.db . Uses PRAGMA statement to read schema. . Describe Schema, here is resource Resource- What is a database schema? it is a layout of the database and its functions . What is the purpose of identity Column in SQL database? the purpose of an identity column in sql database is to make individual unique ids to be able to identify | What is the purpose of a primary key in SQL database? the purpose of a primary key is to be able to access the databse in a specific way with a specific and unique variable | What are the Data Types in SQL table? the data types in an sql table are numeric, date/time, and character string | . import sqlite3 database = &#39;files/sqlite.db&#39; # this is location of database def schema(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Fetch results of Schema results = cursor.execute(&quot;PRAGMA table_info(&#39;users&#39;)&quot;).fetchall() # Print the results for row in results: print(row) # Close the database connection conn.close() schema() . Reading Users table in Sqlite.db . Uses SQL SELECT statement to read data . What is a connection object? After you google it, what do you think it does? represents a unique session with a data source, which I think means that it represents a unique table from one source of data | Same for cursor object?- used to make the connection for executing sql queries, which i think means it is the upon click sort of function | Look at conn object and cursor object in VSCode debugger. What attributes are in the object? | Is &quot;results&quot; an object? How do you know? yes because it has the attributes of the object and it has data | crud functions are also part of the object IN USER CLASS (oop superiority) | . | . import sqlite3 def read(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Execute a SELECT statement to retrieve data from a table results = cursor.execute(&#39;SELECT * FROM users&#39;).fetchall() # Print the results if len(results) == 0: print(&quot;Table is empty&quot;) else: for row in results: print(row) # Close the cursor and connection objects cursor.close() conn.close() read() . Create a new User in table in Sqlite.db . Uses SQL INSERT to add row . Compare create() in both SQL lessons. What is better or worse in the two implementations? i personally prefer sql in the initial lesson because it is less clunky and just generally more streamlined | Explain purpose of SQL INSERT. Is this the same as User init? the purpose is to insert data into a table, which is different from init because it initializes the data (loads it up) | . recursion- menu function calling it self . import sqlite3 def create(): name = input(&quot;Enter your name:&quot;) uid = input(&quot;Enter your user id:&quot;) password = input(&quot;Enter your password&quot;) dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to insert data into a table cursor.execute(&quot;INSERT INTO users (_name, _uid, _password, _dob) VALUES (?, ?, ?, ?)&quot;, (name, uid, password, dob)) # Commit the changes to the database conn.commit() print(f&quot;A new user record {uid} has been created&quot;) except sqlite3.Error as error: print(&quot;Error while executing the INSERT:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #create() . Updating a User in table in Sqlite.db . Uses SQL UPDATE to modify password . Explain try/except, when would except occur? except would occur in specific occasions where there is an error to stop a certain command from running | What code seems to be repeated in each of these examples to point, why is it repeated? there is connect and cursor in all of these, which is basically what starts and bonds everything together | . import sqlite3 def update(): uid = input(&quot;Enter user id to update&quot;) password = input(&quot;Enter updated password&quot;) if len(password) &lt; 2: message = &quot;hacked&quot; password = &#39;gothackednewpassword123&#39; else: message = &quot;successfully updated&quot; # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to update data in a table cursor.execute(&quot;UPDATE users SET _password = ? WHERE _uid = ?&quot;, (password, uid)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: print(f&quot;The row with user id {uid} the password has been {message}&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the UPDATE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #update() . Delete a User in table in Sqlite.db . Uses a delete function to remove a user based on a user input of the id. . Is DELETE a dangerous operation? Why? the user data cannot be recovered once its gone and you might delete more than you wanted to | What is the &quot;f&quot; and {uid} do? the f and uid specifies exactly where and when to send the message and only sends it in specific instances | . import sqlite3 def delete(): uid = input(&quot;Enter user id to delete&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: cursor.execute(&quot;DELETE FROM users WHERE _uid = ?&quot;, (uid,)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: # The uid was found in the table and the row was deleted print(f&quot;The row with uid {uid} was successfully deleted&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the DELETE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #delete() . Menu Interface to CRUD operations . CRUD and Schema interactions from one location by running menu. Observe input at the top of VSCode, observe output underneath code cell. . Why does the menu repeat? the menu repeats because its not streamlined and it needs to define many functions. it could work much faster with a list to have everything in one place. | . def menu(): operation = input(&quot;Enter: (C)reate (R)ead (U)pdate or (D)elete or (S)chema&quot;) if operation.lower() == &#39;c&#39;: create() elif operation.lower() == &#39;r&#39;: read() elif operation.lower() == &#39;u&#39;: update() elif operation.lower() == &#39;d&#39;: delete() elif operation.lower() == &#39;s&#39;: schema() elif len(operation)==0: # Escape Key return else: print(&quot;Please enter c, r, u, or d&quot;) menu() # recursion, repeat menu try: menu() # start menu except: print(&quot;Perform Jupyter &#39;Run All&#39; prior to starting menu&quot;) . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | Create a new Table or do something new, sqlite documentation | In implementation in previous bullet, do you see procedural abstraction? | . yes- data abstraction makes it easier for user to both read and use the data, which is why there are functions that make it easier for user to understand even though they may not be the most functional | .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/16/AP-unit2-4b.html",
            "relUrl": "/2023/03/16/AP-unit2-4b.html",
            "date": " • Mar 16, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "&quot;&quot;&quot; These imports define the key objects &quot;&quot;&quot; from flask import Flask from flask_sqlalchemy import SQLAlchemy from sqlalchemy.exc import IntegrityError &quot;&quot;&quot; These object and definitions are used throughout the Jupyter Notebook. &quot;&quot;&quot; # Setup of key Flask object (app) app = Flask(__name__) # Setup SQLAlchemy object and properties for the database (db) database = &#39;sqlite:///sqlite.db&#39; # path and filename of database app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;] = False app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = database app.config[&#39;SECRET_KEY&#39;] = &#39;SECRET_KEY&#39; db = SQLAlchemy() # This belongs in place where it runs once per project db.init_app(app) . class Snakes(db.Model): __tablename__ = &#39;Snakes&#39; # table name is plural, class name is singular # Define the User schema with &quot;vars&quot; from object id = db.Column(db.Integer, primary_key=True) _name = db.Column(db.String(255), unique=False, nullable=False) _uid = db.Column(db.String(255), unique=True, nullable=False) _snakescore = db.Column(db.String(255), unique=False, nullable=False) # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, snakescore): self._name = name # variables with self prefix become part of the object, self._uid = uid self._snakescore = snakescore # a name getter method, extracts name from object @property def name(self): return self._name # a setter function, allows name to be updated after initial object creation @name.setter def name(self, name): self._name = name # a getter method, extracts email from object @property def uid(self): return self._uid # a setter function, allows name to be updated after initial object creation @uid.setter def uid(self, uid): self._uid = uid # check if uid parameter matches user id in object, return boolean def is_uid(self, uid): return self._uid == uid # dob property is returned as string, to avoid unfriendly outcomes @property def snakescore(self): return self._snakescore # a setter function, allows name to be updated after initial object creation @snakescore.setter def snakescore(self, snakescore): self._snakescore = snakescore # output content using str(object) in human readable form, uses getter # output content using json dumps, this is ready for API response def __str__(self): return json.dumps(self.read()) # CRUD create/add a new record to the table # returns self or None on error def create(self): try: # creates a person object from User(db.Model) class, passes initializers db.session.add(self) # add prepares to persist person object to Users table db.session.commit() # SqlAlchemy &quot;unit of work pattern&quot; requires a manual commit return self except IntegrityError: db.session.remove() return None # CRUD read converts self to dictionary # returns dictionary def read(self): return { &quot;id&quot;: self.id, &quot;name&quot;: self.name, &quot;uid&quot;: self.uid, &quot;snakescore&quot;: self.snakescore } # CRUD update: updates user name, password, phone # returns self def update(self, name=&quot;&quot;, uid=&quot;&quot;, snakescore=&quot;&quot;): &quot;&quot;&quot;only updates values with length&quot;&quot;&quot; if len(name) &gt; 0: self.name = name if len(uid) &gt; 0: self.uid = uid if len(snakescore) &gt; 0: self.snakescore = snakescore db.session.commit() return self # CRUD delete: remove self # None def delete(self): db.session.delete(self) db.session.commit() return None &quot;&quot;&quot;Database Creation and Testing &quot;&quot;&quot; # Builds working data for testing def initSnakes(): with app.app_context(): db.create_all() s1 = Snakes(name=&#39;sabine&#39;, uid=&#39;sab&#39;, snakescore = 10) s2 = Snakes(name=&#39;xxx&#39;, uid=&#39;xxx&#39;, snakescore = 20) s3 = Snakes(name=&quot;bob&quot;, uid=&quot;bobby&quot;, snakescore=30) s4 = Snakes (name=&quot;test&quot;, uid=&quot;testy&quot;, snakescore = 10) snakes = [s1, s2, s3, s4] for snake in snakes: try: &#39;&#39;&#39;add user/post data to table&#39;&#39;&#39; snake.create() except IntegrityError: &#39;&#39;&#39;fails with bad or duplicate data&#39;&#39;&#39; db.session.remove() print(f&quot;Duplicate email, or error: {snake.uid}&quot;) initSnakes() .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/13/table.html",
            "relUrl": "/2023/03/13/table.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "mcq corrections",
            "content": "Tri 2 Final . 41/50 . Anonymous web browsing- I said the activities will not be available for monitors, but the actual answer is that their shopping cart will not be saved. Administrators are seeing the device, not the browser. . | Input data needed by updated system- I said the audio signal and the phone number, but the answer is just the audio signal. . | Algorithm to select two students from thirty- I picked A, but the answer is D. I meant to click D, so i must have misclicked. . | Remove first and last two characters of a string- i only picked one correct answer when there shouldve been two . | values after sequence of boolean expressions- picked c, but the fourth statement makes a false and the fifth makes c false . | modify code to count number of robot moves- i chose b, but that would make the count twice as large as it should be . | why binary search wont work- i picked d, but binary search will work regardless of duplicate elements . | result of call to proc2 procedure- the answer is d because the call to proc1 is first so to you is before happy . | which algorithms run in reasonable time- i chose answer b, but the number of steps is equal to 2n so it doesnt run in reasonable time . | Big Idea 2 Quiz . 24/25 . answer: 1001 0100 i picked: 1101 0100 . this was a reading mistake on my part. i didnt see the second 1. when there is a 0, the 1 moves over and the 0 repeats, meaning there would be the two 0s, but it couldnt go far enough to be 1101 because there is a 1 in between. .",
            "url": "https://butterflyfish.github.io/butterflyfish/test%20corrections/2023/03/13/MCQ-reflection.html",
            "relUrl": "/test%20corrections/2023/03/13/MCQ-reflection.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Unit 2.4a Using Programs with Data, SQLAlchemy",
            "content": "Database and SQLAlchemy . In this blog we will explore using programs with data, focused on Databases. We will use SQLite Database to learn more about using Programs with Data. . College Board talks about ideas like . Program Usage. &quot;iterative and interactive way when processing information&quot; | Managing Data. &quot;classifying data are part of the process in using programs&quot;, &quot;data files in a Table&quot; | Insight &quot;insight and knowledge can be obtained from ... digitally represented information&quot; | Filter systems. &#39;tools for finding information and recognizing patterns&quot; | Application. &quot;the preserve has two databases&quot;, &quot;an employee wants to count the number of book&quot; | . | PBL, Databases, Iterative/OOP . Iterative. Refers to a sequence of instructions or code being repeated until a specific end result is achieved | OOP. A computer programming model that organizes software design around data, or objects, rather than functions and logic | SQL. Structured Query Language, abbreviated as SQL, is a language used in programming, managing, and structuring data | . | . Imports and Flask Objects . Defines and key object creations . Comment on where you have observed these working? | . Flask app object- this is how you can implement the restapi endpoints | SQLAlchemy object- working below! | &quot;&quot;&quot; These imports define the key objects &quot;&quot;&quot; from flask import Flask from flask_sqlalchemy import SQLAlchemy &quot;&quot;&quot; These object and definitions are used throughout the Jupyter Notebook. &quot;&quot;&quot; # Setup of key Flask object (app) app = Flask(__name__) # Setup SQLAlchemy object and properties for the database (db) database = &#39;sqlite:///sqlite.db&#39; # path and filename of database app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;] = False app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = database app.config[&#39;SECRET_KEY&#39;] = &#39;SECRET_KEY&#39; db = SQLAlchemy() # This belongs in place where it runs once per project db.init_app(app) . Model Definition . Define columns, initialization, and CRUD methods for users table in sqlite.db . Comment on these items in the class | class User purpose | db.Model inheritance | init method | @property, @.setter&lt;/li&gt; additional methods | &lt;/ul&gt; db.model is a parameter of the user class (taking something from db model is inheriting) | inheriting/taking from a database and using the functions in that db.model to help us build our user class (db.column, db.integer- need to inherit db.model first to be able to use these) | @property = getter, so you have a getter and a setter | can use CRU easier because you have a function you can call to get and update name (the @property and the setter) | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &quot;&quot;&quot; database dependencies to support sqlite examples &quot;&quot;&quot; import datetime from datetime import datetime import json from json import request from sqlalchemy.exc import IntegrityError from werkzeug.security import generate_password_hash, check_password_hash &#39;&#39;&#39; Tutorial: https://www.sqlalchemy.org/library.html#tutorials, try to get into a Python shell and follow along &#39;&#39;&#39; # Define the User class to manage actions in the &#39;users&#39; table # -- Object Relational Mapping (ORM) is the key concept of SQLAlchemy # -- a.) db.Model is like an inner layer of the onion in ORM # -- b.) User represents data we want to store, something that is built on db.Model # -- c.) SQLAlchemy ORM is layer on top of SQLAlchemy Core, then SQLAlchemy engine, SQL class User(db.Model): __tablename__ = &#39;users&#39; # table name is plural, class name is singular # Define the User schema with &quot;vars&quot; from object id = db.Column(db.Integer, primary_key=True) _name = db.Column(db.String(255), unique=False, nullable=False) _uid = db.Column(db.String(255), unique=True, nullable=False) _password = db.Column(db.String(255), unique=False, nullable=False) _dob = db.Column(db.Date) # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, password=&quot;123qwerty&quot;, dob=datetime.today()): self._name = name # variables with self prefix become part of the object, self._uid = uid self.set_password(password) if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob # a name getter method, extracts name from object @property def name(self): return self._name # a setter function, allows name to be updated after initial object creation @name.setter def name(self, name): self._name = name # a getter method, extracts email from object @property def uid(self): return self._uid # a setter function, allows name to be updated after initial object creation @uid.setter def uid(self, uid): self._uid = uid # check if uid parameter matches user id in object, return boolean def is_uid(self, uid): return self._uid == uid @property def password(self): return self._password[0:10] + &quot;...&quot; # because of security only show 1st characters # update password, this is conventional setter def set_password(self, password): &quot;&quot;&quot;Create a hashed password.&quot;&quot;&quot; self._password = generate_password_hash(password, method=&#39;sha256&#39;) # check password parameter versus stored/encrypted password def is_password(self, password): &quot;&quot;&quot;Check against hashed password.&quot;&quot;&quot; result = check_password_hash(self._password, password) return result # dob property is returned as string, to avoid unfriendly outcomes @property def dob(self): dob_string = self._dob.strftime(&#39;%m-%d-%Y&#39;) return dob_string # dob should be have verification for type date @dob.setter def dob(self, dob): if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob @property def age(self): today = datetime.today() return today.year - self._dob.year - ((today.month, today.day) &lt; (self._dob.month, self._dob.day)) # output content using str(object) in human readable form, uses getter # output content using json dumps, this is ready for API response def __str__(self): return json.dumps(self.read()) # CRUD create/add a new record to the table # returns self or None on error def create(self): try: # creates a person object from User(db.Model) class, passes initializers db.session.add(self) # add prepares to persist person object to Users table db.session.commit() # SqlAlchemy &quot;unit of work pattern&quot; requires a manual commit return self except IntegrityError: db.session.remove() return None # CRUD read converts self to dictionary # returns dictionary def read(self): return { &quot;id&quot;: self.id, &quot;name&quot;: self.name, &quot;uid&quot;: self.uid, &quot;dob&quot;: self.dob, &quot;age&quot;: self.age, } # CRUD update: updates user name, password, phone # returns self def update(self, name=&quot;&quot;, uid=&quot;&quot;, password=&quot;&quot;): &quot;&quot;&quot;only updates values with length&quot;&quot;&quot; if len(name) &gt; 0: self.name = name if len(uid) &gt; 0: self.uid = uid if len(password) &gt; 0: self.set_password(password) db.session.commit() return self # CRUD delete: remove self # None def delete(self): db.session.delete(self) db.session.commit() return None . Initial Data . Uses SQLALchemy db.create_all() to initialize rows into sqlite.db . Comment on how these work? | . Create All Tables from db Object | User Object Constructors | Try / Except | initUsers is the first time the function is actually called, so it stops there first when there is a breakpoint | can use debugging as a process to see the user class functions in action | | . &quot;&quot;&quot;Database Creation and Testing &quot;&quot;&quot; # Builds working data for testing def initUsers(): with app.app_context(): &quot;&quot;&quot;Create database and tables&quot;&quot;&quot; db.create_all() &quot;&quot;&quot;Tester data for table&quot;&quot;&quot; u1 = User(name=&#39;Thomas Edison&#39;, uid=&#39;toby&#39;, password=&#39;123toby&#39;, dob=datetime(1847, 2, 11)) u2 = User(name=&#39;Nikola Tesla&#39;, uid=&#39;niko&#39;, password=&#39;123niko&#39;) u3 = User(name=&#39;Alexander Graham Bell&#39;, uid=&#39;lex&#39;, password=&#39;123lex&#39;) u4 = User(name=&#39;Eli Whitney&#39;, uid=&#39;whit&#39;, password=&#39;123whit&#39;) u5 = User(name=&#39;Indiana Jones&#39;, uid=&#39;indi&#39;, dob=datetime(1920, 10, 21)) u6 = User(name=&#39;Marion Ravenwood&#39;, uid=&#39;raven&#39;, dob=datetime(1921, 10, 21)) u7 = User(name = &#39;sabine&#39;, uid = &#39;fshpeac&#39;, password = &#39;butterflyfish&#39;) users = [u1, u2, u3, u4, u5, u6, u7] &quot;&quot;&quot;Builds sample user/note(s) data&quot;&quot;&quot; for user in users: try: &#39;&#39;&#39;add user to table&#39;&#39;&#39; object = user.create() print(f&quot;Created new uid {object.uid}&quot;) except: # error raised if object nit created &#39;&#39;&#39;fails with bad or duplicate data&#39;&#39;&#39; print(f&quot;Records exist uid {user.uid}, or error.&quot;) initUsers() . Created new uid toby Created new uid niko Created new uid lex Created new uid whit Created new uid indi Created new uid raven Created new uid fshpeac . Check for given Credentials in users table in sqlite.db . Use of ORM Query object and custom methods to identify user to credentials uid and password (Object Relational Model) . takes user input like uid and uses query filter (making it search for something) to look for that input to make sure its correct with what you thought it was/ what its meant to be | check credentials calls on that function and checks to see if user has same uid as the input (which is how find by uid works- it calls check_credentials) | nothing happens if the uid is not found, if it is found it does a second check to check and see if the password is the same, it will return true if both are good (two layers of checking) | | Comment on purpose of following . | . User.query.filter_by- filters the request by a specific variable/piece of information | user.password- verifies the input | def find_by_uid(uid): with app.app_context(): user = User.query.filter_by(_uid=uid).first() return user # returns user object # Check credentials by finding user and verify password def check_credentials(uid, password): # query email and return user record user = find_by_uid(uid) if user == None: return False if (user.is_password(password)): return True return False # check_credentials(&quot;indi&quot;, &quot;123qwerty&quot;) . Create a new User in table in Sqlite.db . Uses SQLALchemy and custom user.create() method to add row. . Comment on purpose of following | . user.find_by_uid() and try/except- do a specific function/find a specific unique value except when there is an error | user = User(...) put a unique uid into the user class | user.dob and try/except try to make unique dob except when error | user.create() and try/except- create a user except when specific error such as it already existing | def create(): # optimize user time to see if uid exists uid = input(&quot;Enter your user id:&quot;) user = find_by_uid(uid) try: print(&quot;Found n&quot;, user.read()) return except: pass # keep going # request value that ensure creating valid object name = input(&quot;Enter your name:&quot;) password = input(&quot;Enter your password&quot;) # Initialize User object before date user = User(name=name, uid=uid, password=password ) # create user.dob, fail with today as dob dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) try: user.dob = datetime.strptime(dob, &#39;%Y-%m-%d&#39;).date() except ValueError: user.dob = datetime.today() print(f&quot;Invalid date {dob} require YYYY-mm-dd, date defaulted to {user.dbo}&quot;) # write object to database with app.app_context(): try: object = user.create() print(&quot;Created n&quot;, object.read()) except: # error raised if object not created print(&quot;Unknown error uid {uid}&quot;) create() . Reading users table in sqlite.db . Uses SQLALchemy query.all method to read data . Comment on purpose of following | . User.query.all - bring all users to be prepared to be read | json_ready assignment - make sure it is readable for user | # SQLAlchemy extracts all users from database, turns each user into JSON def read(): with app.app_context(): table = User.query.all() json_ready = [user.read() for user in table] # each user adds user.read() to list return json_ready read() . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | Add Update functionality to this blog. | Add Delete functionality to this blog. | . def update_user(): &#39;&#39;&#39; Read data for json body &#39;&#39;&#39; body = request.get_json() &#39;&#39;&#39; Avoid garbage in, error checking &#39;&#39;&#39; # validate uid uid = body.get(&#39;uid&#39;) if uid is None or len(uid) &lt; 2: return {&#39;message&#39;: f&#39;User ID is missing, or is less than 2 characters&#39;}, 210 # look for password and dob new_uid = body.get(&#39;uid&#39;) if new_uid is None: return {&#39;message&#39;: f&#39;what are you doing&#39;}, 210 &#39;&#39;&#39; #1: Key code block, lookup USER OBJECT &#39;&#39;&#39; user = User.query.filter_by(uid=uid).first() if user is None: return {&#39;message&#39;: f&#39;User with User ID {uid} not found&#39;}, 210 &#39;&#39;&#39; Additional garbage error checking &#39;&#39;&#39; # set password if provided &#39;&#39;&#39; #2: Key Code block to update user in database &#39;&#39;&#39; # update user in database user.update(uid=new_uid) # success returns json of updated user return jsonify(user.read()) def delete_user(): &#39;&#39;&#39; Read data for json body &#39;&#39;&#39; body = request.get_json() &#39;&#39;&#39; Avoid garbage in, error checking &#39;&#39;&#39; # validate uid uid = body.get(&#39;uid&#39;) if uid is None or len(uid) &lt; 2: return {&#39;message&#39;: f&#39;User ID is missing, or is less than 2 characters&#39;}, 210 &#39;&#39;&#39; #1: Key code block, lookup USER OBJECT &#39;&#39;&#39; user = User.query.filter_by(uid=uid).first() if user is None: return {&#39;message&#39;: f&#39;sorry! record doesnt exist&#39;} &#39;&#39;&#39; #2: Key Code block to delete user from database &#39;&#39;&#39; # delete user from database db.session.delete(user) db.session.commit() # success message return {&#39;message&#39;: f&#39;successfully deleted user with User ID {uid}&#39;} . &lt;/div&gt; | .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/13/AP-unit24a.html",
            "relUrl": "/2023/03/13/AP-unit24a.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "Unit 2.2/2.3 pandas work",
            "content": "from sklearn.datasets import fetch_california_housing import pandas as pd california = fetch_california_housing() df = pd.DataFrame(california.data, columns=california.feature_names) df[&#39;target&#39;] = california.target . import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error # Load the dataset from sklearn.datasets import fetch_california_housing import pandas as pd california = fetch_california_housing() df = pd.DataFrame(california.data, columns=california.feature_names) df[&#39;target&#39;] = california.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(df.drop(&#39;target&#39;, axis=1), df[&#39;target&#39;], test_size=0.3, random_state=42) # Create the linear regression model reg = LinearRegression() # Train the model on the training set reg.fit(X_train, y_train) # Make predictions on the testing set y_pred = reg.predict(X_test) # Calculate the mean squared error of the model mse = mean_squared_error(y_test, y_pred) print(&#39;Mean Squared Error:&#39;, mse) . Mean Squared Error: 0.5305677824766748 . import seaborn as sns from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Load the dataset from sklearn.datasets import fetch_california_housing import pandas as pd california = fetch_california_housing() df = pd.DataFrame(california.data, columns=california.feature_names) df[&#39;target&#39;] = california.target # make sure to define the target (max earnings that the person wants in the area of the house they want to buy) threshold = 2.5 # in units of 100,000 dollars df[&#39;target_binary&#39;] = (df[&#39;target&#39;] &gt;= threshold).astype(int) # split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(df[[&#39;MedInc&#39;, &#39;Population&#39;, &#39;HouseAge&#39;]], df[&#39;target_binary&#39;], test_size=0.3, random_state=42) # create the decision tree classifier model dt = DecisionTreeClassifier() # give the model the training set so it knows what to do dt.fit(X_train, y_train) # use test set to make predictions y_pred = dt.predict(X_test) # find out how accurate it is accuracy = accuracy_score(y_test, y_pred) print(&#39;Accuracy:&#39;, accuracy) # use the model to answer the question (instance) should i buy a house in this area? new_instance = [[1.4, 5000, 20]] # median income, population, house age- ## the median income shows how much people usually make in that area # and when it is below the threshold (aka the budget), it makes sense to buy (as long as the house isnt too old) prediction = dt.predict(new_instance) if prediction == 1: print(&#39;yes buy!&#39;) else: print(&#39;do NOT buy- it is not a good decision!&#39;) response = input(&quot;wanna know why? (yes/no) &quot;) if response == &quot;yes&quot;: print(&quot;this house is either too old, too expensive, or in an overpopulated area.&quot;) else: print(&quot;ok, bye! thanks!&quot;) . /Users/disvs/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names warnings.warn( . Accuracy: 0.7533914728682171 do NOT buy- it is not a good decision! this house is either too old, too expensive, or in an overpopulated area. . BELOW is going to be the information for my pandas dataset! . import seaborn as sns from scipy.stats import mode planets_data = sns.load_dataset(&#39;planets&#39;) print(planets_data.head()) print() print(&quot;--max and min--&quot;) print() print(planets_data[planets_data.mass == planets_data.mass.max()]) print() print(planets_data[planets_data.mass == planets_data.mass.min()]) print() print(&quot;--median--&quot;) print(planets_data[planets_data.mass == planets_data.mass.median()]) print() print(&quot;--mode--&quot;) print() mode_mass = mode(planets_data.mass) print(planets_data[planets_data.mass == mode_mass.mode[0]]) . method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 --max and min-- method number orbital_period mass distance year 321 Radial Velocity 1 2371.0 25.0 37.05 2008 method number orbital_period mass distance year 46 Radial Velocity 1 3.2357 0.0036 1.35 2012 --median-- method number orbital_period mass distance year 347 Radial Velocity 2 572.40 1.26 35.59 2003 556 Radial Velocity 1 456.46 1.26 52.63 2004 600 Radial Velocity 1 1311.00 1.26 33.29 2002 --mode-- method number orbital_period mass distance year 45 Radial Velocity 1 380.800 1.8 20.21 2010 226 Radial Velocity 1 227.000 1.8 44.15 2002 285 Radial Velocity 1 912.000 1.8 121.07 2011 363 Radial Velocity 3 51.645 1.8 64.56 2003 400 Radial Velocity 1 436.900 1.8 150.38 2011 509 Radial Velocity 1 521.000 1.8 114.15 2011 --mean-- Empty DataFrame Columns: [method, number, orbital_period, mass, distance, year] Index: [] . /var/folders/sv/x_7f_68x4vg6tfvg084_dn400000gp/T/ipykernel_50345/290606554.py:19: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning. mode_mass = mode(planets_data.mass) .",
            "url": "https://butterflyfish.github.io/butterflyfish/pandas/2023/03/12/decisiontree.html",
            "relUrl": "/pandas/2023/03/12/decisiontree.html",
            "date": " • Mar 12, 2023"
        }
        
    
  
    
        ,"post7": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . see practice problems for what needs to be cleaned here. . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=False)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 4 324 Junior 4.75 1 246 10 4.00 7 167 10 3.90 0 123 12 3.57 3 469 11 3.45 10 469 11 3.45 5 313 20 3.33 8 235 9th Grade 3.15 6 145 12 2.95 9 nil 9 2.80 2 578 12 2.78 11 456 10 2.75 . I think it&#39;s interesting how you can make it ascend and descend but only have to say the ascending = command once in order to get them to be exact opposites. . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . I think this feature could be very useful for something similar to binary search/ some sort of search engine, specifically for cleaning data because you could weed out certain input data more easily out of larger data sets . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . this feature is useful for finding out the high and low points of a dataset which might make it easier to find a middle point, but it could be hard to use for some other points of data if there is unclean data that messes with the results. . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd dict = { &quot;calories&quot;: [420, 380, 390, 300], &quot;duration&quot;: [50, 40, 45, 42] } print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;, &quot;day4&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 3 300 42 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 day4 300 42 . I think it&#39;s cool how you can have the top part seperating the tables and then define what you want them to be indexed by because it is definitely a better way to visualize the data and really understand what you are looking at . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . this could definitely be useful for cleaning data and making sure data in a large database matches with other data and doesn&#39;t need to be removed . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . it&#39;s interesting to me that it is able to return this information within a function of df, although I am not exactly sure when I would see myself utilizing this data. however, the dtype could help with cleaning data. . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . This could definitely be useful for differentiating between top and bottom points of a large data set/ finding the max and min/ just generally seeing how much space there is between the biggest and smallest samples. It&#39;s also just interesting data to consider . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . knowing how to do this could definitely be vital in incorporating 3rd party data into my own pbl project or into other projects that I am working on if I just want a specific data set. For example, I might use this to incorporate a little box for snake facts into my snake game page . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning Visit Tutorials Point . Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. . Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . | Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. . Decision Trees, prediction by a piecewise constant approximation. | Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . NameError Traceback (most recent call last) /Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb Cell 39 in &lt;cell line: 7&gt;() &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb#X53sZmlsZQ%3D%3D?line=3&#39;&gt;4&lt;/a&gt; from sklearn.metrics import accuracy_score &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb#X53sZmlsZQ%3D%3D?line=5&#39;&gt;6&lt;/a&gt; # Split arrays or matrices into random train and test subsets. -&gt; &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb#X53sZmlsZQ%3D%3D?line=6&#39;&gt;7&lt;/a&gt; X = td.drop(&#39;survived&#39;, axis=1) &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb#X53sZmlsZQ%3D%3D?line=7&#39;&gt;8&lt;/a&gt; y = td[&#39;survived&#39;] &lt;a href=&#39;vscode-notebook-cell:/Users/disvs/vscode/tri2/_notebooks/2023-03-09-AP-unit2-3.ipynb#X53sZmlsZQ%3D%3D?line=8&#39;&gt;9&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) NameError: name &#39;td&#39; is not defined . practice problems! . The data that needs to be cleaned in this example is the Year in School items &quot;9th Grade&quot; and &quot;Junior&quot; because these do not fit in with the rest of the integer options. The gpa &quot;4&quot; is also an outlier because ever other one has two decimal places, making the &quot;3.9&quot; one also in need of cleaning. There is also a grade 20 option there, which is invalid, and a missing Student ID. 4.75 is also larger than the 4 limit so it could need to be cleaned. | Correlation vs causation: while the data shows a sort of trend between sunglasses and ice cream, in actuality these are two unrelated topics that cannot really have a correlation, meaning the data is initially misleading. | reflection on practice problems: . Compression question 3: I said that lossless gives same size file, but the answer was that lossy gives greater reduction in transmission time, which makes sense because the file is less quality and therefore requires less high level transition and saving of pixels. | Q3 and Q4. For Q3 I selected users might search for multiple cities, but I think I misinterpreted what they meant by that. Obviously they can search for as many cities as they want within the data set. For Q4 I picked an option that was not even being collected by the data set in a moment of weakness. The answer has to be the average ticket price because it can determine the popularity and how much people were willing to pay. |",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/09/AP-unit2-3.html",
            "relUrl": "/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post8": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab- the different files have different amount of pixels and different makeup so it affects recreation | Size, height and width, number of pixels- the amount of detail effects how effectively they can be recreated and transferred | Visual perception, lossy compression- lossy compression is where you lose more of the quality of the image, but it may be easier to transfer | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? use ls to see the files that you are trying to access and then you can cd into them | What are the command you use in Windows terminal to access files? dir command | What are some of the major differences? the major difference is that windows doesnt give you the full overview of what you are trying to find | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? path is a big deal because it relates to how the code finds the image | How does the meta data source and label relate to Unit 5 topics? it relates to unit five topics because unit 5 is about processing data and source and label relate to where the data comes from | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? this is interesting in jupyter notebooks because it gives interactive shells so its more helpful for the consumer and allows them to work with different syntaxes | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . Base64 is similar to binary and hexidecimal because it uses numbers (binary) and letters (hexidecimal) but what sets it apart is the way that it combines uppercase, lowercase, letters, numbers, and can even include punctuation in specific instances. It also combines what it tries to convert into sets before it converts it. | U0FC is the first three letters of my name (SAB) in base64, via simple base64 converter code | numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . I have been a consumer of buffering in many instances, most notably when I try to watch something and it takes a decent amount of time for the content to load in and begin playing. | The effects that I have experienced include grey screens with loading images, videos that keep pausing or skipping, and error messages saying that the loading has failed. | These effects apply to images because it may take time for them to load in and in the mean time, it may show an error or loading sign or just a grey/blank screen before the pixels are available. | Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? it takes the pixel value for grey and erases the color | Describe scale image? What is before and after on pixels in three images? before and after- it loses data, but more or less retains the general detail | Is scale image a type of compression? If so, line it up with College Board terms described? yes, it is. you scale the image down, meaning that it has to compress the pixels into a smaller space and therefore some of the fine detail is lost | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (251, 253) Scaled size: (320, 322) -- scaled image -- . grey image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . grey image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . grey image . . lossy vs lossless: images such as the image of the mountain are more lossy because the high detail makes it harder to convert properly, while a simple green square is more lossless because it is lower definition and simpler to transfer . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . GREENSCALE (numpy pixel manipulation) . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_green = self.image_to_html_green() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_green(self): return self._html_green # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_green(self): img_green = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array green_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ green_value = pixel[1] # Use the green channel value if len(pixel) &gt; 3: green_data.append((0, green_value, 0, pixel[3])) # PNG format else: green_data.append((0, green_value, 0)) # end for loop for pixels img_green.putdata(green_data) return self.image_to_html(img_green) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; green image &quot;) display(HTML(ido.html_green)) print() . - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (251, 253) Scaled size: (320, 322) -- scaled image -- . green image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . green image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . green image . .",
            "url": "https://butterflyfish.github.io/butterflyfish/2023/03/07/AP-unit2-2.html",
            "relUrl": "/2023/03/07/AP-unit2-2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Schedule",
          "content": ". Unit 7: Trimester 3 Data Structures . Trimester 3 is taught following college course requirements for CS113 Data Structures. Topics, using Python, include searching, sorting, hashing, algorithms, analysis, object-oriented design, collections, lists, stacks, queues, trees, sets, dictionaries, and graphs. These weeks will also include the a focus on preparations for the May 2nd Create Performance Task and the May 8th Exam. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 25 | | - Unit 2.3 Extracting Information from Data, Pandas - Unit 2.2 Data Compression, Images | | | . 26 | | - Unit 2.4b Using Programs with Data, SQL - Unit 2.4a Using Programs with Data, SQLAlchemy | | | . 27 | | | - Data Structures- Space and Time Complexity | | . 28 | | | - Data Structures- Hashmaps, Sets, Hash Tables, Hashing and Collisions | | . 29 | | | | | . 30 | | | | | . . Unit 1: Introduction to Tools and Resources . The initial weeks focus on introducing Tools, Pair Programming, and the AP Resources that we plan to use throughout the year. At the end of Weeks 0-3, students will be exposed to blogging with GitHub Pages; developing with Jupyter Notebooks, Python, JavaScript, HTML, and Code.org AppLab; working with AP classroom and becoming familiar with Create Performance Task project requirements. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 0 | | - Unit 2.2/2.3 pandas work | | | . 1 | | | | | . 2 | | | | | . 3 | | | | | . . Unit 2: Introduction to Web Development . Websites are made up of several key parts: Frontend, Backend, Data and Deployment. The focus for these weeks is to enable students to perform the aspects of constructing and deploying a simple Website. Fastpages got us started on these concepts, but now we will start building a Website from the ground up. Once again, there will be a lot of learning focused tools and getting things working. But, by the end of the Unit, students will be ready to start many of the technical coding aspects of Web Development, having established a Deployed Website. On Nov 3rd our Trimester work will end with a project and student participation in Electives Department “Night at the Museum” (N@tM). . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 4 | | | | | . 5 | | | - Frontend | | . 6 | | | | | . 7 | | | - fshpeac | | . . Unit 3: N@tM Project, Web, Systems, and Data . The beginning of Trimester 2 is focussed on Algorithms and Coding. Student need to build their own portfolio. That portfolio should focus on their interests in Python and JavaScript programming. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 8 | | | | | . 9 | | | | | . 10 | | | | | . 11 | | | | | . 12 | | | | | . . Unit 4: Algorithmic Programming . Trimester 2 begins with student teaching and a focus on algorithms. Each week a “Student Team” has a teaching assignment supported by College Board materials. Additionally, the Teacher is providing Career Tech mini-labs that correspond to one or more topics for the week. Use the two things together as you improve your learning experience, Frontend blog and Jupyter Notebooks. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 13 | | | | | . 14 | | | | | . 15 | | | | | . 16 | | | | | . . Unit 5: Intro to OOP, Databases, and Backend Programming . Trimester 2 continues with student learning OOP and immediately applying it to Databases. Each week a “Student Teams” will be working on frontend and backend elements for their Create Performance Task (CPT). GitHub pages will be focus of instruction for frontend, OOP in Python will be instruction provided to create a database which will help exceed every CPT requirement. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 17 | | | | | . 18 | | | | | . 19 | | | | | . 20 | | | | | . . Unit 6: Trimester 2 N@tM and finish CPT . Trimester 2 concludes with student presenting their CPT project at N@tM. Each person within “Student Teams” will have their own specialty within the student project that satisfied all their Create Performance Task requirements. Student should be able to talk about design, coding, and present features of their portion of the system. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 21 | | | | | . 22 | | | | | . 23 | | | | | . 24 | | | | | . . Unit 8: Trimester 3 AP Weeks . Trimester 3 contains AP weeks, where activities will be focused on supporting students in test preparations. Day before a test can be used to study for exam in another class. Days that are not focused on test preparation will be focused on planning for year end project/portfolio. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 31 | | | | | . 32 | | | | | . 33 | | | | | . . Unit 9: Trimester 3 Data Structures (continued) . Trimester 3 finishes with a focus on presenting accomplishments for the year at the year end Night at the Museum. These last 2 to 3 weeks will serve as the year end final exam. . . Week Sprint/Points Link AP Test Prep Career Tech Human Prep . 34 | | | | | . 35 | | | | | . 36 | | | | | .",
          "url": "https://butterflyfish.github.io/butterflyfish/schedule",
          "relUrl": "/schedule",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Frontend",
          "content": "| Overview | Tutorial | Calculator | Binary | Grades | Graph | Life | Snake | . sabine (butterflyfish) overview . . welcome to my blog . heres an overview of me. i like coding . | i like backend specifically . | i like fashion . | my name is sabine. surprise. | . . . Code on this blog . This will be all of my homework and classwork for the year (specifically tri 3 data abstractions) . the “ap” tag means its an ap classroom notebook/message | “additional” means it adds on to one of these lessons | “extra” or “personal” is just work i am doing for myself | “recurring” means it is something such as mcq or n@tm reflection that we do more than once | “notes” are just class or EC notes that didn’t fit into another category | .",
          "url": "https://butterflyfish.github.io/butterflyfish/frontend/overview",
          "relUrl": "/frontend/overview",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "fshpeac",
          "content": "| Overview | Covid19 RapidAPI | Jokes API | Users Database | User Login | . fshpeac flop era .",
          "url": "https://butterflyfish.github.io/butterflyfish/api/overview",
          "relUrl": "/api/overview",
          "date": ""
      }
      
  

  

  

  

  
      ,"page6": {
          "title": "about",
          "content": "my name is sabine. im a junior and hoping to do coding as a career. i like analog horror, music, and murder mysteries plus art and medical shows. :3 .",
          "url": "https://butterflyfish.github.io/butterflyfish/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page8": {
          "title": "",
          "content": "yippee . Posts .",
          "url": "https://butterflyfish.github.io/butterflyfish/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://butterflyfish.github.io/butterflyfish/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}